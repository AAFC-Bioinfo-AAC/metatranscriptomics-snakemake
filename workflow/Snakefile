configfile: "config/config.yaml"

from dotenv import load_dotenv
import csv
import os
import sys
import glob 

# Use current working directory to find the project root
PIPELINE_DIR = os.getcwd()
ENV_FILE = os.path.join(PIPELINE_DIR, '.env')
print("Looking for .env at:", ENV_FILE)

# Load the .env file
load_dotenv(ENV_FILE, override=True)

## PROJECT ROOT FROM .env###
PROJECT_ROOT = os.getenv("PROJECT_ROOT")
if not PROJECT_ROOT:
    raise ValueError("PROJECT_ROOT not set in .env")
print("Loaded PROJECT_ROOT:", PROJECT_ROOT)

## Compose all other important paths, always relative to PROJECT_ROOT
DATA_DIR = os.path.join(PROJECT_ROOT, "data")
WORKSPACE_DIR = os.path.join(PROJECT_ROOT, "workspace")
CONFIG_DIR = os.path.join(PIPELINE_DIR, "config")

# Ensure TMPDIR is set, fallback to a default if not
# Construct fallback path with $USER properly resolved
user = os.environ.get("USER")
if not user:
    raise ValueError("Environment variable USER is not set.")

fallback_tmpdir = f"/gpfs/fs7/aafc/scratch/{user}/tmpdir/metaT"
# IMPORTANT: Expand shell vars (e.g. $USER) in TMPDIR if they exist
TMPDIR = os.path.expandvars(os.getenv("TMPDIR", fallback_tmpdir))

print(f"DEBUG: Using TMPDIR = {TMPDIR}")

os.makedirs(TMPDIR, exist_ok=True)

# Ensure RGI_CARD is set, fallback to user CARD DB is no common database is available
env_rgi_card = os.getenv("RGI_CARD", "").strip()
if env_rgi_card:
    RGI_CARD = env_rgi_card
elif "card_latest" in config:
    # Join with PROJECT_ROOT if a relative path
    cfg_card = config["card_latest"]
    RGI_CARD = os.path.join(PROJECT_ROOT, cfg_card) if not os.path.isabs(cfg_card) else cfg_card
else:
    raise ValueError("You must set RGI_CARD in your .env or card_latest in config.yaml!")

if not os.path.exists(RGI_CARD):
    raise ValueError(f"RGI_CARD path does not exist: {RGI_CARD}")

# Early exit if RGI CARD database cannot be loaded.
if not os.path.exists(RGI_CARD):
    sys.exit(f"\nERROR: RGI_CARD not set or path not found: {RGI_CARD}\nEdit .env or config.yaml.\n")
# Check specifically for card_reference.fasta
def has_fasta(path):
    return (
        os.path.exists(os.path.join(path, "card_reference.fasta")) or
        any(name.endswith('.fasta') for name in os.listdir(path))
    )

if not (os.path.exists(f"{RGI_CARD}/card.json")
        and has_fasta(RGI_CARD)):
    sys.exit(
        f"\nERROR: CARD DB not properly prepared at {RGI_CARD} (missing card.json or .fasta).\n"
        "Please follow manual setup instructions (see pipeline README).\n"
    )

## Variable set up
# Join relative config values with PROJECT_ROOT
READS_DIR = os.path.join(PROJECT_ROOT, config["reads_dir"])
TRIMMED_DIR = os.path.join(TMPDIR, config["reads_trimmed"])
HOST_DEP_DIR = os.path.join(PROJECT_ROOT, config["reads_host_dep"])
RRNA_DEP_DIR = os.path.join(PROJECT_ROOT, config["reads_rRNA_dep"])
KRAKEN_OUTPUT_DIR = os.path.join(PROJECT_ROOT, config["taxonomy_short_reads_dir"])
CARD_RGI_OUTPUT_DIR = os.path.join(PROJECT_ROOT, config["amr_screening_dir"])
ASSEMBLIES_DIR = os.path.join(PROJECT_ROOT, config["assemblies_dir"])
RNAQUAST_DIR = os.path.join(PROJECT_ROOT, config["rnaquast_dir"])
LOG_DIR = os.path.join(PROJECT_ROOT, config["log_files"])
BOWTIE_INDEX = os.path.join(PROJECT_ROOT, config["bowtie2_index"])
BOWTIE_INDEX_FILES = [
    f"{BOWTIE_INDEX}.1.bt2",
    f"{BOWTIE_INDEX}.2.bt2",
    f"{BOWTIE_INDEX}.3.bt2",
    f"{BOWTIE_INDEX}.4.bt2",
    f"{BOWTIE_INDEX}.rev.1.bt2",
    f"{BOWTIE_INDEX}.rev.2.bt2"
]
RRNA_DB = config["sortmerna_DB"]
TAXONOMY_DB = config["gtbd_DB"]
BUSCO_LINEAGES = config["busco_lineages"]
LINEAGES = list(BUSCO_LINEAGES.keys())
MEGAHIT_DIR = os.path.join(PROJECT_ROOT, config["co-assembly_dir"])
COASSEMBLY_INDEX = os.path.join(PROJECT_ROOT, config["coassembly_index"])
ASSEMBLY_MAPPING = os.path.join(PROJECT_ROOT, config["sorted_bam_dir"])
PRODIGAL_DIR = os.path.join(PROJECT_ROOT, config["prodigal_dir"])
FEATURECOUNTS_DIR = os.path.join(PROJECT_ROOT, config["featurecounts_dir"])

## SAMPLES 
SAMPLESHEET_PATH = os.path.join(CONFIG_DIR, config["samplesheet"])

SAMPLES = dict()
with open(SAMPLESHEET_PATH, newline='') as csvfile:
    reader = csv.DictReader(csvfile)
    for row in reader:
        fq1 = os.path.join(READS_DIR, row['fastq_1'])  # always resolve relative to reads_dir
        fq2 = os.path.join(READS_DIR, row['fastq_2'])
        SAMPLES[row['sample']] = {
            'fastq_1': fq1,
            'fastq_2': fq2
        }
SAMPLE_NAMES = list(SAMPLES.keys())

## Snakemake modules to include ##
include: "rules/preprocessing.smk"
include: "rules/sortmerna.smk"
include: "rules/taxonomy.smk"
include: "rules/amr_short_reads.smk"

## Rule outputs files ##
rule all:
    input:
        #Reads depleted of host, PhiX and rRNA
        expand(f"{RRNA_DEP_DIR}/{{sample}}_rRNAdep_R1.fastq.gz", sample=SAMPLES),
        expand(f"{RRNA_DEP_DIR}/{{sample}}_rRNAdep_R2.fastq.gz", sample=SAMPLES),
        #Taxonomy with Kraken and Bracken
        #expand(f"{KRAKEN_OUTPUT_DIR}/{{sample}}.report.txt", sample=SAMPLES),
        #expand(f"{KRAKEN_OUTPUT_DIR}/{{sample}}.kraken", sample=SAMPLES),
        #expand(f"{KRAKEN_OUTPUT_DIR}/species/{{sample}}_bracken.species.report.txt", sample=SAMPLES),
        #expand(f"{KRAKEN_OUTPUT_DIR}/genus/{{sample}}_bracken.genus.report.txt", sample=SAMPLES),
        #expand(f"{KRAKEN_OUTPUT_DIR}/phylum/{{sample}}_bracken.phylum.report.txt", sample=SAMPLES),
        # Merged/parsed bracken tables
        #f"{KRAKEN_OUTPUT_DIR}/merged_abundance_species.txt",
        #f"{KRAKEN_OUTPUT_DIR}/merged_abundance_genus.txt",
        #f"{KRAKEN_OUTPUT_DIR}/merged_abundance_phylum.txt",
        #f"{KRAKEN_OUTPUT_DIR}/Bracken_species_raw_abundance.csv",
        #f"{KRAKEN_OUTPUT_DIR}/Bracken_species_relative_abundance.csv",
        #f"{KRAKEN_OUTPUT_DIR}/Bracken_genus_raw_abundance.csv",
        #f"{KRAKEN_OUTPUT_DIR}/Bracken_genus_relative_abundance.csv",
        #f"{KRAKEN_OUTPUT_DIR}/Bracken_phylum_raw_abundance.csv",
        #f"{KRAKEN_OUTPUT_DIR}/Bracken_phylum_relative_abundance.csv",
        # AMR screening with RGI BWT
        #expand(f"{CARD_RGI_OUTPUT_DIR}/{{sample}}_paired.allele_mapping_data.json", sample=SAMPLES),
        #expand(f"{CARD_RGI_OUTPUT_DIR}/{{sample}}_paired.allele_mapping_data.txt", sample=SAMPLES),
        # RNA SPAdes Assemblies and RNAQUAST evaluation with busco lineages
        #expand(f"{ASSEMBLIES_DIR}/{{sample}}.fasta", sample=SAMPLES),
        #expand(f"{RNAQUAST_DIR}/{{sample}}_{{lineage}}", sample=SAMPLES, lineage=LINEAGES),
        # MEGAHIT Coassembly and annotation
        #f"{MEGAHIT_DIR}/final.contigs.fa",
        #f"{PRODIGAL_DIR}/coassembly.faa",
        #f"{PRODIGAL_DIR}/coassembly.fna",
        #f"{PRODIGAL_DIR}/coassembly.gff",
        #f"{PRODIGAL_DIR}/coassembly.saf",
        # Mapping of samples to coassembly, stats, and featurecounts
        #expand(f"{ASSEMBLY_MAPPING}/{{sample}}.coassembly.sorted.bam", sample=SAMPLES),
        #expand(f"{ASSEMBLY_MAPPING}/{{sample}}.flagstat.txt", sample=SAMPLES),
        #expand(f"{ASSEMBLY_MAPPING}/{{sample}}.coverage.txt.gz", sample=SAMPLES),
        #expand(f"{ASSEMBLY_MAPPING}/{{sample}}.idxstats.txt.gz", sample=SAMPLES),
        #expand(f"{FEATURECOUNTS_DIR}/{{sample}}_counts.txt", sample=SAMPLES),
        # Required logs or done markers
        #f"{LOG_DIR}/rgi_reload_db.done"

rule rna_spades:
    input:
        R1 = f"{RRNA_DEP_DIR}/{{sample}}_rRNAdep_R1.fastq.gz",
        R2 = f"{RRNA_DEP_DIR}/{{sample}}_rRNAdep_R2.fastq.gz"
    output:
        fasta = f"{ASSEMBLIES_DIR}/{{sample}}.fasta",
    log:
        f"{LOG_DIR}/spades/{{sample}}.log"
    conda:
        "envs/RNAspades.yaml"
    threads: 48
    params:
        memory = "64000"
    shell:
        r"""
        set -euo pipefail
        echo "Running on: $(hostname)" >> {log}
        echo 'SPAdes version:' >> {log}
        spades.py --version >> {log}

        # Wall time tracking
        start_time=$(date +%s)
        start_hr=$(date)
        echo "Started at: $start_hr" >> {log}

        export TMPDIR="${{TMPDIR:-/tmp}}/spades_{wildcards.sample}_$RANDOM"
        mkdir -p "$TMPDIR" || (echo "Failed to create TMPDIR $TMPDIR" && exit 1)
        echo "Using TMPDIR: $TMPDIR" >> {log}

        
        outdir=$(mktemp -d "$TMPDIR/rnaspades_{wildcards.sample}_XXXXXX")
        spades_tmp="$outdir/tmp"
        mkdir -p "$spades_tmp"

        spades.py --rna \
            -t {threads} \
            -m {params.memory} \
            --tmp-dir "$spades_tmp" \
            -1 {input.R1} \
            -2 {input.R2} \
            -o "$outdir" >> {log} 2>&1 || true

         # Always produce an output
        if [ ! -s "$outdir/transcripts.fasta" ]; then
            echo "Failed to assemble for {wildcards.sample}: transcripts.fasta not produced or empty." >> {log} 2>&1
            echo ">dummy_sequence" > {output.fasta}
        else
            cp "$outdir/transcripts.fasta" {output.fasta}
        fi

        # Wall time logging
        end_time=$(date +%s)
        end_hr=$(date)
        runtime=$((end_time - start_time))
        hours=$((runtime / 3600))
        mins=$(((runtime % 3600) / 60))
        secs=$((runtime % 60))
        echo "Finished at: $end_hr" >> {log}
        echo "Wall time: ${{hours}}h ${{mins}}m ${{secs}}s (total ${{runtime}} seconds)" >> {log}
        echo "Temporary directory to be removed: $outdir" >> {log}
        rm -rf "$outdir"
        """
rule rnaquast_busco:
    input:
        fasta = f"{ASSEMBLIES_DIR}/{{sample}}.fasta",
        busco_lineage = lambda wc: BUSCO_LINEAGES[wc.lineage]
    output:
        report_dir = directory(f"{RNAQUAST_DIR}/{{sample}}_{{lineage}}")
    log:
        f"{LOG_DIR}/rnaquast/{{sample}}_{{lineage}}.log"
    conda: 
        "envs/rnaquast.yaml"
    threads: 4 
    shell:
        r"""
        set -euo pipefail

        echo "Running on: $(hostname)" > {log}
        
        start_time=$(date +%s)
        start_hr=$(date)
        echo "Started at: $start_hr" >> {log}

        rnaQUAST.py --transcripts {input.fasta} \
            --output {output.report_dir} \
            --threads {threads} \
            --busco {input.busco_lineage} \
            &>> {log}
            
        # Wall time logging
        end_time=$(date +%s)
        end_hr=$(date)
        runtime=$((end_time - start_time))
        hours=$((runtime / 3600))
        mins=$(((runtime % 3600) / 60))
        secs=$((runtime % 60))
        echo "Finished at: $end_hr" >> {log}
        echo "Wall time: $hours"h" $mins"m" $secs"s" (total $runtime seconds)" >> {log}
        """
rule megahit_coassembly:
    input: 
        r1 = expand(f"{RRNA_DEP_DIR}/{{sample}}_rRNAdep_R1.fastq.gz", sample=SAMPLES),
        r2 = expand(f"{RRNA_DEP_DIR}/{{sample}}_rRNAdep_R2.fastq.gz", sample=SAMPLES)
    output:
        f"{MEGAHIT_DIR}/final.contigs.fa"
    threads: 48
    conda:
        "envs/megahit.yaml"
    log:
        f"{LOG_DIR}/coassembly/megahit.log"
    shell:
        r"""
        set -euo pipefail

        echo "Running on: $(hostname)" > {log}

        start_time=$(date +%s)
        start_hr=$(date)
        echo "Started at: $start_hr" >> {log}

        r1_list=$(echo {input.r1} | tr ' ' ',')
        r2_list=$(echo {input.r2} | tr ' ' ',')
        
        export TMPDIR="${{TMPDIR:-/tmp}}/megahit_coassembly_${{RANDOM}}_$(date +%s)"
        mkdir -p "$TMPDIR" || (echo "Failed to create TMPDIR $TMPDIR" >> {log}; exit 1)
        echo "Using TMPDIR: $TMPDIR" >> {log}

        megahit \
          -1 "$r1_list" \
          -2 "$r2_list" \
          -t {threads} \
          -o {MEGAHIT_DIR} --force\
          --out-prefix final \
          --tmp-dir "$TMPDIR" \
          > {log} 2>&1

        end_time=$(date +%s)
        end_hr=$(date)
        runtime=$((end_time - start_time))
        hours=$((runtime / 3600))
        mins=$(((runtime % 3600) / 60))
        secs=$((runtime % 60))
        echo "Finished at: $end_hr" >> {log}
        echo "Wall time: $hours"h" $mins"m" $secs"s" (total $runtime seconds)" >> {log}
        echo "Removing tmpdir $TMPDIR" >> {log}
        rm -rf "$TMPDIR"
        """ 
rule index_coassembly:
    input:
        coassembly = f"{MEGAHIT_DIR}/final.contigs.fa"
    output:
        index = [
            f"{COASSEMBLY_INDEX}/coassembly.1.bt2",
            f"{COASSEMBLY_INDEX}/coassembly.2.bt2",
            f"{COASSEMBLY_INDEX}/coassembly.3.bt2",
            f"{COASSEMBLY_INDEX}/coassembly.4.bt2",
            f"{COASSEMBLY_INDEX}/coassembly.rev.1.bt2",
            f"{COASSEMBLY_INDEX}/coassembly.rev.2.bt2"
        ]
    log:
        f"{LOG_DIR}/coassembly/coassembly_index.log"
    threads: 8  
    conda:
        "envs/bowtie2.yaml"  
    params:
        prefix = f"{COASSEMBLY_INDEX}/coassembly"
    shell:
        r"""
        set -euo pipefail
        mkdir -p {COASSEMBLY_INDEX} 

        echo 'Bowtie2 version:' > {log}
        bowtie2 --version >> {log}
        start_time=$(date +%s)
        start_hr=$(date)
        echo "Started at: $start_hr" >> {log}

        bowtie2-build --threads {threads} {input.coassembly} {params.prefix} &>> {log}
        end_time=$(date +%s)
        end_hr=$(date)
        runtime=$((end_time - start_time))
        hours=$((runtime / 3600))
        mins=$(((runtime % 3600) / 60))
        secs=$((runtime % 60))
        echo "Finished at: $end_hr" >> {log}
        echo "Wall time: ${{hours}}h ${{mins}}m ${{secs}}s (total ${{runtime}} seconds)" >> {log}
        """
rule bowtie2_map_transcripts: 
    input:
        index = f"{COASSEMBLY_INDEX}/coassembly.1.bt2",
        r1 = f"{RRNA_DEP_DIR}/{{sample}}_rRNAdep_R1.fastq.gz",
        r2 = f"{RRNA_DEP_DIR}/{{sample}}_rRNAdep_R2.fastq.gz"
    output:
        bam = f"{ASSEMBLY_MAPPING}/{{sample}}.coassembly.sorted.bam"
    log:
        f"{LOG_DIR}/sorted_bam/{{sample}}_sorted.log"
    threads: 16
    conda:
        "envs/bowtie2.yaml"
    params:
         index_prefix = f"{COASSEMBLY_INDEX}/coassembly"
    shell:
        r"""
        set -euo pipefail

        echo 'Bowtie2 version:' > {log}
        bowtie2 --version >> {log}
        echo 'Samtools version:' >> {log}
        samtools --version | head -n 1 >> {log}
        start_time=$(date +%s)
        start_hr=$(date)
        echo "Started at: $start_hr" >> {log}

        bowtie2 -x {params.index_prefix} -1 {input.r1} -2 {input.r2} --local -p {threads} 2>> {log}\
        | samtools view -bS - 2>> {log} \
        | samtools sort -@ {threads} -o {output.bam} 2>> {log}
        end_time=$(date +%s)
        end_hr=$(date)
        runtime=$((end_time - start_time))
        hours=$((runtime / 3600))
        mins=$(((runtime % 3600) / 60))
        secs=$((runtime % 60))
        echo "Finished at: $end_hr" >> {log}
        echo "Wall time: ${{hours}}h ${{mins}}m ${{secs}}s (total ${{runtime}} seconds)" >> {log}
        """
rule assembly_stats_depth:
    input:
        bam = f"{ASSEMBLY_MAPPING}/{{sample}}.coassembly.sorted.bam"
    output:
        stats = f"{ASSEMBLY_MAPPING}/{{sample}}.flagstat.txt",
        depth = f"{ASSEMBLY_MAPPING}/{{sample}}.coverage.txt.gz",
        idxstats = f"{ASSEMBLY_MAPPING}/{{sample}}.idxstats.txt.gz"
    threads: 2
    shell:
        """
        samtools flagstat {input.bam} > {output.stats} &&
        samtools depth {input.bam} | pigz -p {threads} > {output.depth} &&
        samtools idxstats {input.bam} | pigz -p {threads} > {output.idxstats}
        """
rule prodigal_genes:
    input:
        coassembly = f"{MEGAHIT_DIR}/final.contigs.fa"
    output:
        proteins = f"{PRODIGAL_DIR}/coassembly.faa",
        nucs = f"{PRODIGAL_DIR}/coassembly.fna",
        gff = f"{PRODIGAL_DIR}/coassembly.gff",
        saf = f"{PRODIGAL_DIR}/coassembly.saf"
    threads: 1
    log:
        f"{LOG_DIR}/prodigal/coassembly_prodigal.log"
    conda:
        "envs/prodigal.yaml"
    shell:
        r"""
        set -euo pipefail

        # Ensure output directory exists
        mkdir -p {PRODIGAL_DIR}

        # Wall time tracking
        start_time=$(date +%s)
        start_hr=$(date)
        echo "Started at: $start_hr" > {log}

        echo 'prodigal version:' >> {log}
        prodigal -v >> {log} 2>&1


        prodigal -i {input.coassembly} \
            -a {output.proteins} \
            -d {output.nucs} \
            -o {output.gff} \
            -p meta \
            -f gff 2>&1 | grep -v "^Finding genes in sequence" | grep -v "^Request:" >> {log}

        awk '$3=="CDS" {{OFS="\t"; split($9,a,";"); for(i in a){{if(a[i]~/^ID=/)id=substr(a[i],4)}}; print id, $1, $4, $5, $7}}' {output.gff} | \
        awk 'BEGIN{{print "GeneID\tChr\tStart\tEnd\tStrand"}} 1' > {output.saf}

        # Wall time logging
        end_time=$(date +%s)
        end_hr=$(date)
        runtime=$((end_time - start_time))
        hours=$((runtime / 3600))
        mins=$(((runtime % 3600) / 60))
        secs=$((runtime % 60))
        echo "Finished at: $end_hr" >> {log}
        echo "Wall time: $hours"h" $mins"m" $secs"s" (total $runtime seconds)" >> {log}
        """
rule featurecounts:
    input:
        saf = f"{PRODIGAL_DIR}/coassembly.saf",
        bam = f"{ASSEMBLY_MAPPING}/{{sample}}.coassembly.sorted.bam"
    output:
        counts = f"{FEATURECOUNTS_DIR}/{{sample}}_counts.txt"
    threads: 4
    log:
        f"{LOG_DIR}/featurecounts/{{sample}}_featurecounts.log"
    conda:
        "envs/featurecounts.yaml"
    shell:
        r"""
        set -euo pipefail

        # Ensure output directory exists
        mkdir -p {FEATURECOUNTS_DIR}

        # Wall time tracking
        start_time=$(date +%s)
        start_hr=$(date)
        echo "Started at: $start_hr" > {log}

        echo 'featureCounts version:' >> {log}
        featureCounts -v >> {log}

        featureCounts -a {input.saf} -F SAF -p -T {threads} -o {output.counts} {input.bam} &>> {log}

        # Wall time logging
        end_time=$(date +%s)
        end_hr=$(date)
        runtime=$((end_time - start_time))
        hours=$((runtime / 3600))
        mins=$(((runtime % 3600) / 60))
        secs=$((runtime % 60))
        echo "Finished at: $end_hr" >> {log}
        echo "Wall time: $hours"h" $mins"m" $secs"s" (total $runtime seconds)" >> {log}
        """